{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PDF Preprocessing and Embedding Pipeline\n",
        "\n",
        "Replica do fluxo de `main.py` com geração adicional de embeddings usando `sentence-transformers/all-MiniLM-L6-v2`. Execute as células sequencialmente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e8c1f35b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# (Opcional) Instale dependências necessárias\n",
        "%pip install --quiet \"unstructured[pdf]\" sentence-transformers pymupdf4llm pdfminer.six pi-heif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2eba3fb5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PDF path: documents/Edital-Processo-Seletivo-Inteli_-Graduacao-2026_AJUSTADO.pdf\n",
            "JSON output: documents/Edital-Processo-Seletivo-Inteli_-Graduacao-2026_AJUSTADO-output.json\n",
            "Chunks output: documents/Edital-Processo-Seletivo-Inteli_-Graduacao-2026_AJUSTADO-chunks.json\n",
            "Embeddings output: documents/Edital-Processo-Seletivo-Inteli_-Graduacao-2026_AJUSTADO-embeddings.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "from unstructured.staging.base import elements_to_json\n",
        "import pymupdf4llm\n",
        "\n",
        "file_path = Path('documents')\n",
        "base_file_name = 'Edital-Processo-Seletivo-Inteli_-Graduacao-2026_AJUSTADO'\n",
        "pdf_path = file_path / f'{base_file_name}.pdf'\n",
        "json_output_path = file_path / f'{base_file_name}-output.json'\n",
        "chunks_output_path = file_path / f'{base_file_name}-chunks.json'\n",
        "embeddings_output_path = file_path / f'{base_file_name}-embeddings.json'\n",
        "print(f'PDF path: {pdf_path}')\n",
        "print(f'JSON output: {json_output_path}')\n",
        "print(f'Chunks output: {chunks_output_path}')\n",
        "print(f'Embeddings output: {embeddings_output_path}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Clean and normalize text content.\"\"\"\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.replace('\\ufb01', 'fi')\n",
        "    text = text.replace('\\ue009', 'tt')\n",
        "    text = re.sub(r'^\\d+$', '', text)\n",
        "    text = re.sub(r'[•◦▪▫]', '•', text)\n",
        "    return text.strip()\n",
        "\n",
        "def determine_hierarchy_level(element: dict) -> str:\n",
        "    element_type = element.get('type')\n",
        "    text = element.get('text', '')\n",
        "    if element_type == 'Title':\n",
        "        if re.match(r'^\\d+\\.', text):\n",
        "            level = len(text.split('.')[0])\n",
        "            return f'level_{level}'\n",
        "        return 'title_main'\n",
        "    if element_type == 'ListItem':\n",
        "        return 'list_item'\n",
        "    return 'body'\n",
        "\n",
        "def extract_section_info(element: dict) -> str:\n",
        "    text = element.get('text', '')\n",
        "    element_type = element.get('type')\n",
        "    if element_type == 'Title' and re.match(r'^\\d+\\.', text):\n",
        "        return text.split('.')[0] + '.' + text.split('.')[1].strip() if '.' in text else text\n",
        "    if element_type == 'ListItem' and re.match(r'^\\d+\\.', text):\n",
        "        return text\n",
        "    return 'general'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_chunk_metadata(chunk_elements):\n",
        "    if not chunk_elements:\n",
        "        return {}\n",
        "    return {\n",
        "        'chunk_size': len(' '.join(chunk_elements)),\n",
        "        'element_count': len(chunk_elements),\n",
        "        'chunk_type': 'mixed'\n",
        "    }\n",
        "\n",
        "def extract_enhanced_metadata(element: dict) -> dict:\n",
        "    metadata = element.get('metadata', {})\n",
        "    return {\n",
        "        'element_id': element.get('element_id'),\n",
        "        'element_type': element.get('type'),\n",
        "        'page_number': metadata.get('page_number'),\n",
        "        'parent_id': metadata.get('parent_id'),\n",
        "        'text_length': len(element.get('text', '')),\n",
        "        'is_header': element.get('type') in ['Title'],\n",
        "        'is_list_item': element.get('type') == 'ListItem',\n",
        "        'is_table_content': element.get('type') in ['Table', 'TableRow'],\n",
        "        'hierarchy_level': determine_hierarchy_level(element),\n",
        "        'section': extract_section_info(element)\n",
        "    }\n",
        "\n",
        "def preprocess_elements(elements):\n",
        "    processed_elements = []\n",
        "    for element in elements:\n",
        "        text = element.get('text', '').strip()\n",
        "        if not text or len(text) < 10:\n",
        "            continue\n",
        "        if element.get('type') == 'Footer':\n",
        "            continue\n",
        "        cleaned_text = clean_text(text)\n",
        "        if not cleaned_text:\n",
        "            continue\n",
        "        processed_elements.append({\n",
        "            'text': cleaned_text,\n",
        "            'metadata': extract_enhanced_metadata(element),\n",
        "            'original_element': element\n",
        "        })\n",
        "    return processed_elements\n",
        "\n",
        "def create_contextual_chunks(processed_elements, max_tokens=400):\n",
        "    chunks = []\n",
        "    current_section = 'Introduction'\n",
        "    current_subsection = ''\n",
        "    for element in processed_elements:\n",
        "        text = element['text']\n",
        "        element_type = element['metadata']['element_type']\n",
        "        if element_type == 'Title' and any(char.isdigit() for char in text[:5]):\n",
        "            current_section = text\n",
        "            current_subsection = ''\n",
        "        elif element_type == 'Title':\n",
        "            current_subsection = text\n",
        "        chunk_metadata = {\n",
        "            **element['metadata'],\n",
        "            'section': current_section,\n",
        "            'subsection': current_subsection,\n",
        "            'document_type': 'admission_notice',\n",
        "            'language': 'portuguese'\n",
        "        }\n",
        "        chunks.append({'text': text, 'metadata': chunk_metadata})\n",
        "    return chunks\n",
        "\n",
        "def optimize_chunks(chunks, target_size=300):\n",
        "    optimized = []\n",
        "    i = 0\n",
        "    while i < len(chunks):\n",
        "        current_chunk = chunks[i]\n",
        "        current_text = current_chunk['text']\n",
        "        current_metadata = current_chunk['metadata']\n",
        "        j = i + 1\n",
        "        while (j < len(chunks) and len(current_text) < target_size and\n",
        "               chunks[j]['metadata']['section'] == current_metadata['section']):\n",
        "            combined_text = current_text + ' ' + chunks[j]['text']\n",
        "            if len(combined_text) <= target_size * 1.5:\n",
        "                current_text = combined_text\n",
        "                j += 1\n",
        "            else:\n",
        "                break\n",
        "        optimized.append({'text': current_text, 'metadata': current_metadata})\n",
        "        i = j\n",
        "    return optimized\n",
        "\n",
        "def create_semantic_chunks(elements, max_chunk_size=512):\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_size = 0\n",
        "    for element in elements:\n",
        "        element_text = element.get('text', '').strip()\n",
        "        if not element_text:\n",
        "            continue\n",
        "        element_type = element.get('type')\n",
        "        if element_type in ['Title'] and current_chunk:\n",
        "            chunks.append({'text': ' '.join(current_chunk), 'metadata': get_chunk_metadata(current_chunk)})\n",
        "            current_chunk = [element_text]\n",
        "            current_size = len(element_text)\n",
        "        else:\n",
        "            if current_size + len(element_text) > max_chunk_size and current_chunk:\n",
        "                chunks.append({'text': ' '.join(current_chunk), 'metadata': get_chunk_metadata(current_chunk)})\n",
        "                current_chunk = [element_text]\n",
        "                current_size = len(element_text)\n",
        "            else:\n",
        "                current_chunk.append(element_text)\n",
        "                current_size += len(element_text)\n",
        "    if current_chunk:\n",
        "        chunks.append({'text': ' '.join(current_chunk), 'metadata': get_chunk_metadata(current_chunk)})\n",
        "    return chunks\n",
        "\n",
        "def preprocess_for_embedding(json_file_path):\n",
        "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
        "        elements = json.load(f)\n",
        "    print(f'Loaded {len(elements)} elements from JSON')\n",
        "    processed_elements = preprocess_elements(elements)\n",
        "    print(f'Preprocessed {len(processed_elements)} elements')\n",
        "    chunks = create_contextual_chunks(processed_elements)\n",
        "    print(f'Created {len(chunks)} initial chunks')\n",
        "    optimized_chunks = optimize_chunks(chunks)\n",
        "    print(f'Optimized to {len(optimized_chunks)} chunks')\n",
        "    embedding_ready_chunks = []\n",
        "    for i, chunk in enumerate(optimized_chunks):\n",
        "        embedding_ready_chunks.append({\n",
        "            'id': f'chunk_{i}',\n",
        "            'content': chunk['text'],\n",
        "            'metadata': chunk['metadata']\n",
        "        })\n",
        "    return embedding_ready_chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ensure_partitioned_pdf(pdf_path: Path, output_path: Path) -> None:\n",
        "    if output_path.exists():\n",
        "        print('JSON file already exists, skipping PDF extraction')\n",
        "        return\n",
        "    if not pdf_path.exists():\n",
        "        raise FileNotFoundError(f'PDF file not found: {pdf_path}')\n",
        "    print('Extracting elements from PDF...')\n",
        "    elements = partition_pdf(filename=str(pdf_path))\n",
        "    elements_to_json(elements=elements, filename=str(output_path))\n",
        "    print('PDF extraction completed')\n",
        "\n",
        "def generate_embeddings(chunks, model_name='sentence-transformers/all-MiniLM-L6-v2', normalize=True):\n",
        "    if not chunks:\n",
        "        print('No chunks available for embedding.')\n",
        "        return []\n",
        "    print(f'Loading embedding model: {model_name}')\n",
        "    model = SentenceTransformer(model_name)\n",
        "    texts = [chunk['content'] for chunk in chunks]\n",
        "    embeddings = model.encode(\n",
        "        texts,\n",
        "        batch_size=32,\n",
        "        show_progress_bar=True,\n",
        "        normalize_embeddings=normalize\n",
        "    )\n",
        "    chunks_with_embeddings = []\n",
        "    for chunk, vector in zip(chunks, embeddings):\n",
        "        enriched = dict(chunk)\n",
        "        enriched['embedding'] = vector.tolist()\n",
        "        chunks_with_embeddings.append(enriched)\n",
        "    print(f'Generated embeddings for {len(chunks_with_embeddings)} chunks')\n",
        "    return chunks_with_embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JSON file already exists, skipping PDF extraction\n",
            "Starting preprocessing pipeline...\n",
            "Loaded 1050 elements from JSON\n",
            "Preprocessed 922 elements\n",
            "Created 922 initial chunks\n",
            "Optimized to 269 chunks\n",
            "✅ Created 269 chunks ready for embedding\n",
            "✅ Saved chunks to: documents/Edital-Processo-Seletivo-Inteli_-Graduacao-2026_AJUSTADO-chunks.json\n",
            "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 9/9 [00:05<00:00,  1.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated embeddings for 269 chunks\n",
            "✅ Saved chunk embeddings to: documents/Edital-Processo-Seletivo-Inteli_-Graduacao-2026_AJUSTADO-embeddings.json\n"
          ]
        }
      ],
      "source": [
        "# Pipeline execution\n",
        "ensure_partitioned_pdf(pdf_path, json_output_path)\n",
        "print('Starting preprocessing pipeline...')\n",
        "embedding_chunks = preprocess_for_embedding(json_output_path)\n",
        "with open(chunks_output_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(embedding_chunks, f, ensure_ascii=False, indent=2)\n",
        "print(f'✅ Created {len(embedding_chunks)} chunks ready for embedding')\n",
        "print(f'✅ Saved chunks to: {chunks_output_path}')\n",
        "embedded_chunks = generate_embeddings(embedding_chunks)\n",
        "with open(embeddings_output_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(embedded_chunks, f, ensure_ascii=False, indent=2)\n",
        "print(f'✅ Saved chunk embeddings to: {embeddings_output_path}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Chunk 1 ---\n",
            "ID: chunk_0\n",
            "Content: 1. Curso 2. Público-Alvo 3. Calendário 4. Inscrições 4.1.Taxa de inscrição 4.1.1.Solicitação de Isenção da taxa de inscrição 4.2.Política de reembolso 4.3.Confirmação de Inscrição...\n",
            "Content Length: 179\n",
            "Section: Introduction\n",
            "Element Type: ListItem\n",
            "Page: 2\n",
            "Embedding dims: 384\n",
            "\n",
            "--- Chunk 2 ---\n",
            "ID: chunk_1\n",
            "Content: 5. Sobre o Processo Seletivo 5.1.Eixo Prova 5.1.1.Quantidade de questões e formato 5.1.2.Uso de materiais de apoio 5.1.3.Duração Eixo Prova 5.1.4.Dinâmica da Prova Inteli 5.1.5.Eixo Prova - Formato On...\n",
            "Content Length: 514\n",
            "Section: Introduction\n",
            "Element Type: ListItem\n",
            "Page: 2\n",
            "Embedding dims: 384\n",
            "\n",
            "--- Chunk 3 ---\n",
            "ID: chunk_2\n",
            "Content: 6. Critérios de Avaliação e Desclassificação 6.1.Eixo Prova 6.1.1.Formato Online 6.1.2.Formato Presencial 6.2.Eixo Perfil 6.2.1.Redações e Atividades Extracurriculares 6.3.Eixo Projeto 7. Bolsas e Fin...\n",
            "Content Length: 284\n",
            "Section: Introduction\n",
            "Element Type: ListItem\n",
            "Page: 2\n",
            "Embedding dims: 384\n",
            "\n",
            "Total chunks: 269\n",
            "Total characters: 67334\n",
            "Average chunk size: 250.3 characters\n"
          ]
        }
      ],
      "source": [
        "# Preview first chunks\n",
        "for i, chunk in enumerate(embedded_chunks[:3]):\n",
        "    print(f'\\n--- Chunk {i + 1} ---')\n",
        "    print(f\"ID: {chunk['id']}\")\n",
        "    print(f\"Content: {chunk['content'][:200]}...\")\n",
        "    print(f\"Content Length: {len(chunk['content'])}\")\n",
        "    print(f\"Section: {chunk['metadata'].get('section', 'N/A')}\")\n",
        "    print(f\"Element Type: {chunk['metadata'].get('element_type', 'N/A')}\")\n",
        "    print(f\"Page: {chunk['metadata'].get('page_number', 'N/A')}\")\n",
        "    print(f\"Embedding dims: {len(chunk['embedding'])}\")\n",
        "total_chars = sum(len(chunk['content']) for chunk in embedded_chunks)\n",
        "avg_chunk_size = total_chars / len(embedded_chunks) if embedded_chunks else 0\n",
        "print(f'\\nTotal chunks: {len(embedded_chunks)}')\n",
        "print(f'Total characters: {total_chars}')\n",
        "print(f'Average chunk size: {avg_chunk_size:.1f} characters')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10 acer- tos: 1 chunks\n",
            "10 acertos: 9 chunks\n",
            "10.1. Calendário de Matrícula: 1 chunks\n",
            "10.3. Vagas remanescentes: 17 chunks\n",
            "10.5. Indeferimento de matrícula: 3 chunks\n",
            "10.6. Cancelamento de matrícula: 26 chunks\n",
            "100 pontos: 4 chunks\n",
            "11 acer- tos: 1 chunks\n",
            "11 acertos: 6 chunks\n",
            "12 acer- tos: 1 chunks\n",
            "12 acertos: 9 chunks\n",
            "12/10/2025, solicitando nova confirmação.: 2 chunks\n",
            "13 acertos: 6 chunks\n",
            "14 acertos: 6 chunks\n",
            "14 acertos 20 acertos: 1 chunks\n",
            "15 acertos: 6 chunks\n",
            "16 acertos: 1 chunks\n",
            "18 acertos: 1 chunks\n",
            "19 acer- tos: 1 chunks\n",
            "19/10/2025 das 9h às 11h Realização do Eixo Prova Formato Remoto: 4 chunks\n",
            "1ª chamada: 2 chunks\n",
            "20 acer- tos: 4 chunks\n",
            "20 acertos: 5 chunks\n",
            "2ª chamada: 2 chunks\n",
            "3ª chamada: 8 chunks\n",
            "4.1. Taxa de Inscrição: 2 chunks\n",
            "4.1.1. Solicitação de isenção da taxa de inscrição: 6 chunks\n",
            "4.3. Confirmação de inscrição: 2 chunks\n",
            "5.1. Eixo Prova: 4 chunks\n",
            "5.1.1. Quantidade de questões e formato: 4 chunks\n",
            "5.1.2. Uso de materiais de apoio: 2 chunks\n",
            "5.1.3. Duração Eixo Prova: 1 chunks\n",
            "5.1.4. Dinâmica da Prova Inteli: 6 chunks\n",
            "5.1.5. Eixo Prova - Formato Online: 3 chunks\n",
            "5.1.6. Eixo Prova - Formato Presencial: 1 chunks\n",
            "5.1.7.4. Olimpíadas de Matemática: 5 chunks\n",
            "5.1.8. Convocação para Eixo Perfil: 2 chunks\n",
            "5.2. Eixo Perfil: 2 chunks\n",
            "5.2.1. Redações: 5 chunks\n",
            "5.2.2. Atividades Extracurriculares, Honras e Méritos: 7 chunks\n",
            "5.3. Eixo Projeto: 5 chunks\n",
            "5.4. Acessibilidade: 3 chunks\n",
            "6.1. Eixo Prova: 1 chunks\n",
            "6.1.1. Formato Online: 1 chunks\n",
            "6.1.2. Formato Presencial: 1 chunks\n",
            "6.2. Eixo Perfil: 2 chunks\n",
            "6.2.1. Redações e Atividades Extracurriculares: 2 chunks\n",
            "6.3. Eixo Projeto: 1 chunks\n",
            "7.1 Programa de bolsas: 4 chunks\n",
            "7.2 Financiamento Estudantil Inteli: 3 chunks\n",
            "7.2.1 Elegibilidade e Solicitação: 4 chunks\n",
            "8.1. Nota final: 3 chunks\n",
            "8.1.Nota Final: 1 chunks\n",
            "8.2. Nota de Corte: 2 chunks\n",
            "8.2.Nota de Corte: 1 chunks\n",
            "8.3. Equivalência aos exames ENEM, SAT, ACT, IB ou Olimpíadas de Matemática.: 2 chunks\n",
            "8.3.4. IB - International Baccalaureate: 1 chunks\n",
            "8.3.5. Olimpíadas de Matemática: 1 chunks\n",
            "8.3.Equivalência aos exames ENEM, SAT, ACT, IB ou Olimpíadas de Matemática: 2 chunks\n",
            "8.4. Critérios de desempate: 1 chunks\n",
            "8.5. Equidade, unicidade e irrevogabilidade de critérios: 3 chunks\n",
            "Introduction: 3 chunks\n",
            "Pág.22 Pág.22: 1 chunks\n",
            "Pág.23 Pág.24 Pág.24 Pág.25 Pág.26 Pág.26 Pág.26: 17 chunks\n",
            "até 10 pontos: 2 chunks\n",
            "até 20 pontos: 2 chunks\n",
            "até 30 pontos: 3 chunks\n",
            "até 40 pontos: 2 chunks\n",
            "e 40%, conforme análise.: 1 chunks\n",
            "às 14:30h, horário de Brasília.: 15 chunks\n"
          ]
        }
      ],
      "source": [
        "# Section distribution overview\n",
        "sections = {}\n",
        "for chunk in embedded_chunks:\n",
        "    section = chunk['metadata'].get('section', 'Unknown')\n",
        "    sections[section] = sections.get(section, 0) + 1\n",
        "for section, count in sorted(sections.items()):\n",
        "    print(f'{section}: {count} chunks')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Markdown export saved to output.md\n"
          ]
        }
      ],
      "source": [
        "# Convert the document to markdown using pymupdf4llm\n",
        "md_text = pymupdf4llm.to_markdown(str(pdf_path))\n",
        "Path('output.md').write_bytes(md_text.encode('utf-8'))\n",
        "print('✅ Markdown export saved to output.md')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
